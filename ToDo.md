# ToDo
<!-- templates -->
<!-- ## 日付     -->
<!-- ## 目標     -->
<!-- ## 内容     -->
<!-- ## 次の目標 -->

## 2018/07/31
## 目標
- [ ] CNNの派生形を調査
- [x] kerasによるモデル作成
- [ ] 研修とセミナーについて
- [x] 支払依頼など
## 内容
### CNN派生形
- FCN(全層畳み込み)
    CNNの最後の全結合層を畳み込み層に置き換えるというもの。  
    全結合層を用いると、出力は分類クラスであるが、畳み込み層を用いることで、
    出力を二次元マップにできる。  
    この手法(二次元画像からpredictionマップを得る手法)によって得られる利点は、
    - 任意のサイズの画像の入力が可能になる(全結合層に入力するサイズは指定される)
    - conv層の被りがなくなる(よくわからない)
    の2つのメリットが得られる。  
    ただし、ストライドが2以上のフィルタを用いた場合には、解像度が落ちてしまうというデメリットも生じる。  
    対策として、Deconvolution層によって解像度の拡大を行っている。  
    スキップアーキテクチャ  
    CNNの畳み込み層の下位レイヤー(ローカルな特徴マップ)を上位レイヤー(高次の特徴マップ)と結合させて精度の向上を図ったもの(誤差逆伝搬にもいい影響あり)  
    Deconvolutionとは、convolution後の特徴マップを引き延ばして(空白を間に入れる)カーネルを適用し、新たな特徴マップを得る手法  

- Dilated/Atrous畳み込み  
    普通の畳み込みは、カーネルはそれぞれ隣り合うピクセルを参照しているが、
    Dilated畳み込みでは、1つ開けて隣のピクセルを参照するなど、より広い範囲を参照しての畳み込みを行う。  
    - CRF(条件付き確率場)後処理
- R-CNN(Regionsなのでリカレントでない、あくまで空間)
    入力画像から、物体が映っている領域の候補を抽出(色や濃淡勾配などの特徴が類似している領域)し、CNNのインプットに合うようにリサイズし、
    それぞれの領域に対してCNNで特徴量を計算し、分類する。  
- SPP(spatial pyramid pooling)
    異なるサイズの入力に対して、出力サイズを一定にするプーリング手法  
    画像(特徴マップ)を格子状に1, 4, 16, ...と分割し、その中で、プーリングを行う。
    その後、1+4+16+...と、つなげたベクトルをSPPの出力とする。
    つまり、出力のサイズはピラミッドのサイズとチャンネル数にのみ依存する。

### kerasのモデル作成
- cifar10データセットの認識モデル(畳み込み)
## 次の目標
- wslでGPU認識できたと思ったらできてなかった(重要)
- DockerでUbuntu環境作成(GPUが使えるらしい)
- 現在はSpyderにコードを移して実行している(やや不便)
プロキシの問題でデータセットをダウンロードできない場合の応急処置
import urllib.request
# proxy の設定
proxy_support = urllib.request.ProxyHandler({'http' : 'http://user:pass@10.1.8.72:8080/',
                                             'https': 'https://user:pass@10.1.8.72:8080/'})
opener = urllib.request.build_opener(proxy_support)
urllib.request.install_opener(opener)

## 2018/07/31
## 目標
### vim
- [x] vimでのマークダウンのプレビュー
### python
- [x] kerasのチュートリアルなど進める
## 内容
### bash
- CUDA, CuDNN, Tensorflow, kerasの導入(割としんどかった)
### vim
- :Previmによってクローム上でプレビュー可能に(bashrcいじった)
- previmのプレビュー画面をgithub調に
- Ctrl+j(insertモード)で改行(文の途中であれば、それを無視して下の行から)
### python
- kerasでmnistのチュートリアルというかは2層のモデルの作成、学習と評価の写経

## 次の目標

